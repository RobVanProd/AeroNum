<div align="center">
  <h1>AeroNum v0.3.0</h1>
  <p><strong>Production-Ready Numerical & Deep-Learning Substrate</strong></p>
  
  [![GitHub stars](https://img.shields.io/github/stars/RobVanProd/AeroNum.svg?style=social&label=Star)](https://github.com/RobVanProd/AeroNum)
  [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
  [![CI Benchmarks](https://github.com/RobVanProd/AeroNum/actions/workflows/benchmark.yml/badge.svg)](https://github.com/RobVanProd/AeroNum/actions)
</div>

AeroNum provides **Rust-level safety, C-level speed, and Python-level ergonomics for AI**. It is the official computational backend for the Aero systems programming language, designed structurally to eliminate the two-language problem in deep learning.

## üöÄ Live Ecosystem

- **[Interactive WebAssembly Playground](https://github.com/RobVanProd/AeroNum/tree/main/playground)**: Try AeroNum and AeroNN entirely in your browser. Zero-cost limits evaluated dynamically without local installation.
- **[AeroNum Benchmarking Dashboard](https://github.com/RobVanProd/AeroNum/tree/main/benchmarks/dashboard)**: Live telemetry proving our mathematically precise performance boundaries.
- **[Documentation & Guides](https://github.com/RobVanProd/AeroNum/tree/main/docs)**: From "NumPy to AeroNum" and full API references.

## ‚ö° Performance Highlights

Powered by intrinsic zero-cost abstractions, linear memory types, and a standard library designed for ML:
- **`‚â• 1.4x` Speedup over PyTorch** in End-to-End GPT-2 Transformer sequence generation natively.
- **`‚â• 5.3x` Speedup traversing CPU to GPU** on massive 4096x4096 MatMul calculations automatically mapping `.to("cuda")`.
- **`0` Garbage Collection Pauses**: Memory tracking utilizes pure AST static graph optimizations resolving bounds with OS-native speeds.

## üß† Why Aero Wins

1. **Zero-Cost Topology**: Neural iterations trace explicitly across `aero::vec` limits directly without Garbage Collection tracing memory allocations. Overhead delays typically costing 15%-20% throughput structurally drop exactly to hardware limits.
2. **Cross-Device Memory Security**: Aero asserts ownership natively verifying CUDA allocations, enforcing that references cannot be dropped randomly onto Host targets.
3. **Ergonomic Parity**: Syntactic layout maps directly to PyTorch standard limits (`optimizer.step()`, `loss.backward()`) without the Python interpreter boundaries!

## üì¶ Quick Start

### 1. Try it out instantly
**[Click here to open the Aero Playground](#)** and train a neural network right in your browser!

### 2. Connect via `aero-pkg`

Ensure you have the Aero compiler installed. In your project's `aero.toml`:

```toml
[dependencies]
aeronum = "0.3.0"
aeronn = "0.3.0"
aeronum-gpu = "0.3.0"
```

### 3. Build a Transformer & MLP

```rust
use aeronum::{Array};
use aeronn::{Transformer, Dense, Sequential};

fn main() {
    // 1. MLP execution
    let mut mlp = Sequential::new();
    mlp.add(Box::new(Dense::new(64, 128)));
    
    // 2. Transformer Flagship (6 Layers, 6 Heads, 384 Dim)
    let mut model = Transformer::new(6, 384, 6);
    
    // 3. Dispatch to Nvidia GPU 
    model.to("cuda"); 
    
    // Train at C-level speeds!
    println("Aero Flagship NLP Model Deployed!");
}
```

## üó∫Ô∏è Roadmap to v1.0.0 (Q2‚ÄìQ3 2026)
- [ ] Distributed Training (Multi-GPU & Node scaling mappings) (Q2)
- [ ] Direct Quantization Interfaces (INT8/FP8 native unrolling) (Q2)
- [ ] Improved Compiler Diagnostics & Borrow Checker Errors (Q2)
- [ ] Formal Language Specification (Q3)
- [ ] Advanced Graph Compilation (Kernel Fusion API) (Q3)
- [ ] Profiler API hooks & Native Flamegraphs (Q3)
- [ ] LLVM-backend Optimization parity (Q3)
- [ ] aero-pkg Central Registry (registry.aero) (Q3)

## üìÑ License
This project is licensed under the MIT License - see the LICENSE file for details.
